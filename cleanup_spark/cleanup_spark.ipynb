{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f6985e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import boto3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.client import Config\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy.engine.url import URL\n",
    "load_dotenv()\n",
    "\n",
    "driver=os.environ.get('POSTGRES_DRIVER') # Postgres JDBC driver path\n",
    "host=os.environ.get('POSTGRES_HOST') # Postgres host url\n",
    "port=os.environ.get('POSTGRES_PORT') # Postgres port\n",
    "database=os.environ.get('POSTGRES_DATABASE') # Postgres database\n",
    "username=os.environ.get('POSTGRES_USERNAME') # Postgres username\n",
    "password=os.environ.get('POSTGRES_PASSWORD') # Postgres Password\n",
    "staging_schema=os.environ.get('POSTGRES_STAGING_SCHEMA') # Staging schema\n",
    "analytics_schema=os.environ.get('POSTGRES_ANALYTICS_SCHEMA') # Staging schema\n",
    "sparkClassPath=os.environ.get('PYSPARK_SUBMIT_ARGS') \n",
    "\n",
    "# The postgres driver needs some dependencies, so call it\n",
    "# directly from the package from maven using spark.jars.packages. See link:\n",
    "# https://www.reddit.com/r/apachespark/comments/qhv03n/error_javalangclassnotfoundexception/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d88910",
   "metadata": {},
   "source": [
    "### Configure spark session and specify classpath config for reading and writing to postgres database\n",
    "\n",
    "##### Use the command `wget https://jdbc.postgresql.org/download/postgresql-42.6.0.jar  `  to download the postgresql jar file and save the jar file to the pysaprk jars directory of your python environment that is :  \n",
    "`/Users/akawiifeanyicourage/opt/anaconda3/lib/python3.9/site-packages/pyspark/jars/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfcf7653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/21 03:58:42 WARN Utils: Your hostname, Akawi-Ifeanyi-Courage-Data-Engineer.local resolves to a loopback address: 127.0.0.1; using 172.20.10.2 instead (on interface en0)\n",
      "23/06/21 03:58:42 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/opt/homebrew/Cellar/apache-spark/3.3.1/libexec/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/akawiifeanyicourage/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/akawiifeanyicourage/.ivy2/jars\n",
      "org.postgresql#postgresql added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-8f92874e-6d82-4634-bcdd-c823dd9524d5;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.postgresql#postgresql;42.6.0 in central\n",
      "\tfound org.checkerframework#checker-qual;3.31.0 in central\n",
      ":: resolution report :: resolve 68ms :: artifacts dl 2ms\n",
      "\t:: modules in use:\n",
      "\torg.checkerframework#checker-qual;3.31.0 from central in [default]\n",
      "\torg.postgresql#postgresql;42.6.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-8f92874e-6d82-4634-bcdd-c823dd9524d5\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/3ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/21 03:58:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = (SparkSession\n",
    "        .builder\n",
    "        .config(\"spark.executor.extraClassPath\", sparkClassPath)\n",
    "        .master('local[*]')\n",
    "        .appName('data2bots')\n",
    "        .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fe0ba7",
   "metadata": {},
   "source": [
    "### Download the CSV files using boto3 Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cb6615a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download all the files using boto3\n",
    "s3 = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n",
    "bucket_name = \"d2b-internal-assessment-bucket\"\n",
    "response = s3.list_objects(Bucket=bucket_name, Prefix=\"orders_data\")\n",
    "files_to_download = ['orders.csv','reviews.csv','shipment_deliveries.csv']\n",
    "\n",
    "for file in files_to_download:\n",
    "    s3.download_file(bucket_name, f\"orders_data/{file}\", f\"{file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b926e153",
   "metadata": {},
   "source": [
    "### Read the downloaded files into Pyspark Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b942834c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the csv files into spark dataframes\n",
    "orders_schema = \"order_id INT, customer_id INT, order_date DATE, product_id STRING, \\\n",
    "unit_price INT, quantity INT,amount INT\"\n",
    "\n",
    "reviews_schema = \"review INT, product_id INT\"\n",
    "\n",
    "shipment_deliveries_schema = \"shipment_id INT, order_id INT, shipment_date DATE, delivery_date DATE\"\n",
    "\n",
    "\n",
    "orders_df = (spark\n",
    "            .read\n",
    "            .format('csv')\n",
    "            .option('header', 'true')\n",
    "            .option('mode', 'FAILFAST')\n",
    "            .schema(orders_schema)\n",
    "            .load('orders.csv')\n",
    "            )\n",
    "\n",
    "reviews_df = (spark\n",
    "            .read\n",
    "            .format('csv')\n",
    "            .option('header', 'true')\n",
    "            .option('mode', 'FAILFAST')\n",
    "            .schema(reviews_schema)\n",
    "            .load('reviews.csv')\n",
    "            )\n",
    "\n",
    "shipment_deliveries_df = (spark\n",
    "            .read\n",
    "            .format('csv')\n",
    "            .option('header', 'true')\n",
    "            .option('mode', 'FAILFAST')\n",
    "            .schema(shipment_deliveries_schema)\n",
    "            .load('shipment_deliveries.csv')\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8554f29",
   "metadata": {},
   "source": [
    "### Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae4d7680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- order_date: date (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- unit_price: integer (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- amount: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90d995e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/21 03:58:50 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: order_id, customer_id, order_date, product_id, unit_price, quantity, total_price\n",
      " Schema: order_id, customer_id, order_date, product_id, unit_price, quantity, amount\n",
      "Expected: amount but found: total_price\n",
      "CSV file: file:///Users/akawiifeanyicourage/Desktop/data_engineering_projects/data_2_bots_de_assessment/cleanup/cleanup_spark/orders.csv\n",
      "+--------+-----------+----------+----------+----------+--------+------+\n",
      "|order_id|customer_id|order_date|product_id|unit_price|quantity|amount|\n",
      "+--------+-----------+----------+----------+----------+--------+------+\n",
      "|       1|          5|2022-07-13|        24|       139|      10|  1390|\n",
      "|       2|         14|2021-04-06|         2|       273|       4|  1092|\n",
      "|       3|         17|2022-07-29|        20|       253|       9|  2277|\n",
      "|       4|         14|2022-08-27|         8|       334|       1|   334|\n",
      "|       5|         25|2021-12-15|         6|       334|       3|  1002|\n",
      "+--------+-----------+----------+----------+----------+--------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b1122d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- review: integer (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reviews_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72e18856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "|review|product_id|\n",
      "+------+----------+\n",
      "|     1|        21|\n",
      "|     3|         1|\n",
      "|     2|         8|\n",
      "|     1|         5|\n",
      "|     5|        22|\n",
      "+------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reviews_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66bb339d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- shipment_id: integer (nullable = true)\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- shipment_date: date (nullable = true)\n",
      " |-- delivery_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "shipment_deliveries_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92396f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+-------------+-------------+\n",
      "|shipment_id|order_id|shipment_date|delivery_date|\n",
      "+-----------+--------+-------------+-------------+\n",
      "|          1|       1|   2022-07-14|         null|\n",
      "|          2|       2|         null|         null|\n",
      "|          3|       3|   2022-07-31|   2022-08-03|\n",
      "|          4|       4|   2022-09-02|   2022-09-05|\n",
      "|          5|       5|   2021-12-19|   2021-12-20|\n",
      "+-----------+--------+-------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "shipment_deliveries_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfc9d80",
   "metadata": {},
   "source": [
    "### Write the raw data to Postgres database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7aae4e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/21 03:58:52 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: order_id, customer_id, order_date, product_id, unit_price, quantity, total_price\n",
      " Schema: order_id, customer_id, order_date, product_id, unit_price, quantity, amount\n",
      "Expected: amount but found: total_price\n",
      "CSV file: file:///Users/akawiifeanyicourage/Desktop/data_engineering_projects/data_2_bots_de_assessment/cleanup/cleanup_spark/orders.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# write orders data to postgres database\n",
    "(orders_df.write\n",
    " .format('jdbc')\n",
    " .option('url', f'{host}:{port}/{database}')\n",
    " .option('driver', driver)\n",
    " .option('dbtable', f'{staging_schema}.orders')\n",
    " .option('user', username)\n",
    " .option('password', password)\n",
    " .mode('overwrite')\n",
    ".save())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0bc1ccb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# write reviews data to postgres database\n",
    "(reviews_df.write\n",
    " .format('jdbc')\n",
    " .option('url', f'{host}:{port}/{database}')\n",
    " .option('driver', driver)\n",
    " .option('dbtable', f'{staging_schema}.reviews')\n",
    " .option('user', username)\n",
    " .option('password', password)\n",
    " .mode('overwrite')\n",
    ".save())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "861eaa78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# write shipment_deliveries data to postgres database\n",
    "(shipment_deliveries_df.write\n",
    " .format('jdbc')\n",
    " .option('url', f'{host}:{port}/{database}')\n",
    " .option('driver', driver)\n",
    " .option('dbtable', f'{staging_schema}.shipment_deliveries')\n",
    " .option('user', username)\n",
    " .option('password', password)\n",
    " .mode('overwrite')\n",
    ".save())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
